# ES Text Chunk Manager Utility (`chunk_manager.py`)

## File: `src/logllm/utils/chunk_manager.py`

### Overview

Utility class designed to fetch large amounts of text data (like log lines associated with an event or file) from Elasticsearch and serve it in manageable chunks that respect LLM token limits.

### Class: `ESTextChunkManager`

- **Purpose**: Given an identifier (like a file ID or event ID), fetches all associated documents from a specified ES index, and provides methods to iterate through the content field (`field`) of these documents in chunks, ensuring each chunk's token count (calculated via `len_fn`) does not exceed `max_len`.
- **Key Methods**:
  - **`__init__(self, id: Any, field: str, index: str, db: ElasticsearchDatabase)`**: Initializes by fetching _all_ relevant hits using `_get_all_hits` (which uses `db.scroll_search`).
  - **`get_next_chunk(self, max_len: int, len_fn: Callable[[str], int]) -> str`**: Returns the next chunk of aggregated text content. Internally uses `_build_chunk` which dynamically adjusts how many hits' content are combined to stay under `max_len`. Updates internal pointers. Returns empty string when done.
  - **`is_end(self) -> bool`**: Returns `True` if all fetched hits have been processed into chunks.
  - **`get_current_chunk(self) -> str | None`**: Returns the last chunk generated by `get_next_chunk`.
  - **`_get_all_hits() -> list`**: Fetches all documents matching the ID using scrolling.
  - **`_build_chunk(...) -> str`**: Iteratively adds content from hits to the chunk, adjusting the number added per step based on token limits.
