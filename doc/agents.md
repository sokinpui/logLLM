# Detailed Documentation for Agent-Related Files

This document describes the agents and related utility classes used within the `logLLM` project, primarily located under `src/logllm/agents/`.

## File: `src/logllm/agents/agent_abc.py`

### Class: `Agent(ABC)`
- **Purpose**: Abstract base class defining the interface for all agents in the system, particularly those potentially using graph-based workflows (like `langgraph`).
- **Key Methods**:
  - **`_build_graph(self, typed_state) -> CompiledStateGraph`**
    - **Description**: Abstract method intended for subclasses that implement a `langgraph.StateGraph`. Should construct and compile the agent's workflow graph.
    - **Parameters**:
      - `typed_state`: Type definition (e.g., `TypedDict`) for the agentâ€™s state.
    - **Returns**: A compiled `langgraph` graph (`CompiledGraph` or similar).
    - **Usage**: Implemented by graph-based agents to define their logic flow.
  - **`run(self)`**
    - **Description**: Abstract method to execute the agent synchronously. Subclasses must implement this to define their primary execution logic.
    - **Usage**: `result = agent.run(input_data)`
  - **`arun(self)`**
    - **Description**: Abstract method to execute the agent asynchronously.
    - **Usage**: `result = await agent.arun(input_data)`

- **Utility Function**: **`add_string_message(left: list[str], right: str | list[str]) -> list[str]`**
  - **Purpose**: Helper function, typically used with `langgraph`'s `Annotated` state fields, to append new messages (string or list of strings) to an existing list of messages within the agent's state.
  - **Parameters**:
    - `left` (list[str]): The existing list of messages in the state.
    - `right` (str | list[str]): The new message or messages to append.
  - **Returns**: The updated list of messages.
  - **Usage**: Facilitates state updates in graph-based agents, e.g., `messages: Annotated[list[str], add_string_message]`.

---

## File: `src/logllm/agents/parser_agent.py`

This file contains agents responsible for parsing log files stored on the local filesystem into structured CSV format using Grok patterns.

### Class: `SimpleGrokLogParserState(TypedDict)`
- **Purpose**: Defines the state dictionary structure used by `SimpleGrokLogParserAgent`.
- **Fields**:
  - `log_file_path` (str): Path to the input log file.
  - `grok_pattern` (Optional[str]): The Grok pattern to use. Can be provided or generated by LLM.
  - `output_csv_path` (str): Path where the output CSV file should be saved (populated on success).
  - `sample_logs` (str): Sample log lines used for LLM pattern generation (populated by agent if needed).
  - `parsed_lines` (int): Count of lines successfully matched and parsed by the Grok pattern.
  - `skipped_lines` (int): Count of lines that did not match the Grok pattern.

### Class: `GrokPatternSchema(BaseModel)`
- **Purpose**: Pydantic schema used to instruct the LLM to return only the Grok pattern string when generating patterns.
- **Fields**:
  - `grok_pattern` (str): Field to hold the generated Grok pattern.

### Class: `SimpleGrokLogParserAgent`
- **Purpose**: Parses a single log file using a provided or LLM-generated Grok pattern and outputs a CSV file.
- **Key Methods**:
  - **`__init__(self, model: LLMModel)`**
    - **Description**: Initializes the agent with an LLM model instance and a `PromptsManager` instance to fetch prompts for pattern generation.
    - **Parameters**:
      - `model` (LLMModel): The language model instance.
    - **Usage**: `agent = SimpleGrokLogParserAgent(model)`
  - **`run(self, initial_state: SimpleGrokLogParserState, show_progress: bool = False) -> SimpleGrokLogParserState`**
    - **Description**: Main execution method. Takes the initial state, generates a Grok pattern via LLM if needed, runs the Grok parser, and returns the final state including the output path and statistics. `show_progress` has minimal effect here.
    - **Parameters**:
      - `initial_state` (SimpleGrokLogParserState): Input state containing the log file path and optionally a grok pattern.
      - `show_progress` (bool): Flag (mainly for compatibility, less impact on Grok).
    - **Returns**: The final `SimpleGrokLogParserState` after parsing.
    - **Usage**: `result_state = agent.run({"log_file_path": "path/to.log"})`
  - **`_generate_grok_pattern(self, log_file_path: str) -> Optional[str]`**
    - **Description**: Samples the log file, uses the LLM via `PromptsManager` to generate a Grok pattern based on the samples, and validates the LLM response format.
    - **Parameters**:
      - `log_file_path` (str): Path to the log file to sample.
    - **Returns**: The generated Grok pattern string or `None` on failure.
    - **Usage**: Internal method called by `run` if no pattern is provided.
  - **`_run_grok_parser(self, state: SimpleGrokLogParserState) -> SimpleGrokLogParserState`**
    - **Description**: Takes the state (including the Grok pattern), attempts to compile the pattern, reads the log file line by line, matches against the pattern, collects parsed data, dynamically determines CSV headers, and writes the output CSV. Updates `parsed_lines` and `skipped_lines` in the state.
    - **Parameters**:
      - `state` (SimpleGrokLogParserState): The state containing file path and Grok pattern.
    - **Returns**: The updated state including `output_csv_path` on success, or empty path on failure.
    - **Usage**: Internal method called by `run` after ensuring a pattern exists.

### Class: `GroupLogParserAgent`
- **Purpose**: Orchestrates the parsing of log files belonging to different groups (as defined in the database). It can operate sequentially or leverage multiple worker processes (using `SimpleGrokLogParserAgent`) for parallel parsing.
- **Key Methods**:
  - **`__init__(self, model: LLMModel)`**
    - **Description**: Initializes the agent with an LLM model (used for sequential runs or pattern pre-determination) and a database connection.
    - **Parameters**:
      - `model` (LLMModel): The language model instance.
    - **Usage**: `group_agent = GroupLogParserAgent(model)`
  - **`fetch_groups(self) -> Optional[Dict[str, List[str]]]`**
    - **Description**: Queries the Elasticsearch index specified by `cfg.INDEX_GROUP_INFOS` to retrieve the mapping of group names to lists of associated log file paths.
    - **Returns**: A dictionary `{group_name: [file_path1, file_path2, ...]}` or `None` on error.
    - **Usage**: Called by `run` to determine which files to parse.
  - **`parse_all_logs(self, groups: Dict[str, List[str]], num_threads: int, show_progress: bool) -> Dict[str, List[str]]]`**
    - **Description**: Manages the parsing workflow. It attempts to pre-determine a Grok pattern for each group using the first available file. It then distributes the parsing tasks (either sequentially or using a `ProcessPoolExecutor`) to workers (`_parse_file_worker`), passing the pre-determined pattern (if available) and the `show_progress` flag. It collects the paths of successfully generated CSV files.
    - **Parameters**:
      - `groups` (Dict[str, List[str]]): The group-to-files mapping.
      - `num_threads` (int): Number of parallel worker processes (1 for sequential).
      - `show_progress` (bool): Controls whether detailed progress is shown (sequential) or suppressed (parallel, usually).
    - **Returns**: A dictionary `{group_name: [output_csv_path1, output_csv_path2, ...]}` containing paths to successfully created CSVs.
    - **Usage**: Internal method called by `run`.
  - **`_update_progress_bar(...)`**
    - **Description**: Helper method to display a simple progress bar during sequential execution when `show_progress` is `False`.
    - **Usage**: Provides visual feedback for long sequential runs.
  - **`run(self, num_threads: int = 1, show_progress: bool = False) -> dict`**
    - **Description**: Main entry point for the group parser. Fetches groups from the database and calls `parse_all_logs` to perform the parsing.
    - **Parameters**:
      - `num_threads` (int): Number of parallel workers to use.
      - `show_progress` (bool): Flag to control progress display.
    - **Returns**: The results dictionary from `parse_all_logs`.
    - **Usage**: `results = group_agent.run(num_threads=4)`

- **Worker Function**: **`_parse_file_worker(file_path: str, group_grok_pattern: Optional[str], show_progress: bool) -> Tuple[str, Optional[str]]`**
  - **Purpose**: Function executed by each worker process in parallel mode. It initializes its own `SimpleGrokLogParserAgent`, runs parsing for a single file using the provided group pattern (or generates one if needed), includes fallback logic to try file-specific generation if the group pattern fails, and returns the original path and the output CSV path (or None).
  - **Parameters**:
    - `file_path` (str): Path of the log file for this worker.
    - `group_grok_pattern` (Optional[str]): The pre-determined Grok pattern for the group (can be `None`).
    - `show_progress` (bool): Passed down, but less relevant in parallel workers.
  - **Returns**: Tuple `(original_file_path, output_csv_path or None)`.
  - **Usage**: Used internally by `parse_all_logs` with `ProcessPoolExecutor`.

---

## File: `src/logllm/agents/es_parser_agent.py`

This file contains agents responsible for parsing log data directly from Elasticsearch indices, handling pattern generation, validation, retries, and indexing results back into Elasticsearch.

### Class: `ScrollGrokParserState(TypedDict)`
- **Purpose**: Defines the state/configuration for the `ScrollGrokParserAgent`. This agent handles the actual scrolling, parsing, and batch indexing.
- **Fields**:
  - `source_index`, `target_index`, `failed_index` (str): ES index names.
  - `grok_pattern` (str): Pattern to use for parsing.
  - `field_to_parse` (str): Field in source docs containing raw log lines.
  - `source_query` (Optional[Dict]): ES query to select source documents.
  - `fields_to_copy` (Optional[List[str]]): Additional fields to copy from source to target.
  - `batch_size` (int): Number of documents per bulk indexing request.
  - `is_fallback_run` (bool): Flag indicating if this run should use the fallback pattern (store originals in failed_index).
  - *Result fields (populated by agent)*: `processed_count`, `successfully_indexed_count`, `failed_indexed_count`, `parse_error_count`, `index_error_count`, `status` (str).

### Class: `SingleGroupParseGraphState(TypedDict)`
- **Purpose**: Defines the state for the `langgraph`-based `SingleGroupParserAgent` orchestrator.
- **Fields**:
  - *Configuration*: `group_name`, `source_index`, `target_index`, `failed_index`, `field_to_parse`, `fields_to_copy`, `sample_size_generation`, `sample_size_validation`, `validation_threshold`, `batch_size`, `max_regeneration_attempts`, `keep_unparsed_index`, `provided_grok_pattern`.
  - *Dynamic State*: `current_attempt`, `current_grok_pattern`, `last_failed_pattern`, `sample_lines_for_generation`, `sample_lines_for_validation`, `validation_passed`, `final_parsing_status`, `final_parsing_results_summary` (dict), `error_messages` (list).

### Class: `AllGroupsParserState(TypedDict)`
- **Purpose**: Defines the state for the `AllGroupsParserAgent`, which manages parsing across multiple groups.
- **Fields**:
  - `group_info_index` (str): Index containing group definitions.
  - `field_to_parse` (str): Source field name.
  - `fields_to_copy` (Optional[List[str]]): Fields to copy.
  - `group_results` (Dict[str, SingleGroupParseGraphState]): Dictionary storing the final state result for each processed group.
  - `status` (str): Overall status ('pending', 'running', 'completed', 'failed').

### Class: `ScrollGrokParserAgent`
- **Purpose**: Low-level agent responsible for scrolling through documents in a source Elasticsearch index, applying a Grok pattern, and performing bulk indexing of successful parses to a target index and failed/fallback documents to a separate failed index. Also stores parsing results summary to a history index.
- **Key Methods**:
  - **`__init__(self, db: ElasticsearchDatabase)`**
    - **Description**: Initializes with a database connection.
  - **`run(self, state: ScrollGrokParserState) -> ScrollGrokParserState`**
    - **Description**: Main execution method. Initializes Grok (if not fallback), uses `db.scroll_and_process_batches` to iterate through source documents, calls `_process_batch` for each batch, flushes remaining data, and returns the final state with counts and status.
    - **Parameters**: `state` (ScrollGrokParserState): Configuration for the run.
    - **Returns**: Updated `ScrollGrokParserState` with results.
  - **`_initialize_grok(self, pattern: str) -> bool`**: Compiles the Grok pattern.
  - **`_process_single_hit(...) -> Literal["success", "parse_failed", "skip"]`**: Parses a single ES hit, appends to internal success or failed batch lists based on match or fallback mode.
  - **`_process_batch(...) -> bool`**: Processes a list of hits from the scroll, updates counts, and triggers batch flushes if needed. Returns `True` to continue scrolling.
  - **`_flush_success_batch(self, target_index: str)`**: Performs bulk update/upsert operation for successfully parsed documents to the `target_index`.
  - **`_flush_failed_batch(self, failed_index: str)`**: Performs bulk index operation for failed/fallback documents (storing original source) to the `failed_index`.

### Class: `SingleGroupParserAgent(Agent)`
- **Purpose**: Orchestrates the parsing process for a *single* log group using a `langgraph` workflow. Handles pattern generation (via LLM), validation, retries, and invokes `ScrollGrokParserAgent` for the actual parsing and indexing based on the validated pattern or fallback. Stores results in a history index.
- **Key Methods**:
  - **`__init__(self, model: LLMModel, db: ElasticsearchDatabase, prompts_manager: PromptsManager)`**
    - **Description**: Initializes the agent, its dependencies (model, db, prompts), the sub-agent (`ScrollGrokParserAgent`), and builds the LangGraph workflow.
  - **`run(self, initial_config: Dict[str, Any]) -> SingleGroupParseGraphState`**
    - **Description**: Takes an initial configuration dictionary, transforms it into the required `SingleGroupParseGraphState`, invokes the compiled LangGraph, and returns the final state dictionary after the graph execution completes.
    - **Parameters**: `initial_config` (dict): Configuration passed from the orchestrator (e.g., `AllGroupsParserAgent`).
    - **Returns**: The final `SingleGroupParseGraphState`.
  - **`_build_graph(self) -> CompiledGraph`**: Defines the nodes and edges of the LangGraph state machine (start -> [generate | validate] -> validate -> [parse | retry] -> fallback -> store_results -> END).
  - **Graph Nodes** (`_start_node`, `_generate_grok_node`, `_validate_pattern_node`, `_parse_all_node`, `_fallback_node`, `_prepare_for_retry_node`, `_store_results_node`): Implement the logic for each step in the workflow, updating the `SingleGroupParseGraphState`.
  - **Conditional Edges** (`_decide_pattern_source`, `_decide_after_generate`, `_decide_after_validation`): Determine the next node based on the current state (e.g., pattern provided, validation success/failure, retries remaining).

### Class: `AllGroupsParserAgent`
- **Purpose**: Orchestrates the parsing process across *all* log groups defined in the database. It fetches group information and uses a thread/process pool to run `SingleGroupParserAgent` instances concurrently for each group.
- **Key Methods**:
  - **`__init__(self, model: LLMModel, db: ElasticsearchDatabase, prompts_manager: PromptsManager)`**
    - **Description**: Initializes with shared dependencies (model, db, prompts manager).
  - **`run(self, initial_state: AllGroupsParserState, num_threads: int, batch_size: int, sample_size: int, ...) -> AllGroupsParserState`**
    - **Description**: Main execution method. Fetches all groups, prepares configuration dictionaries for each group, submits tasks to a `ThreadPoolExecutor` (which run `_parallel_group_worker_new`), collects the final `SingleGroupParseGraphState` results for each group, and returns the overall `AllGroupsParserState`.
    - **Parameters**:
      - `initial_state` (AllGroupsParserState): Basic initial state.
      - `num_threads` (int): Number of parallel workers.
      - *Other parameters*: Batch sizes, sample sizes, thresholds, retries, flags (`keep_unparsed_index`) passed down to workers.
    - **Returns**: The final `AllGroupsParserState` containing results for all groups.
  - **`_get_all_groups(self, group_info_index: str) -> List[Dict[str, Any]]`**: Fetches group definitions from ES.

- **Worker Function**: **`_parallel_group_worker_new(single_group_config: Dict[str, Any], prompts_json_path: str) -> Tuple[str, SingleGroupParseGraphState]`**
  - **Purpose**: Function executed by each worker thread/process. It initializes its *own* dependencies (DB, Model, PromptsManager), instantiates a `SingleGroupParserAgent`, runs its graph using the provided configuration, and returns the group name and the final state object. Handles critical errors within the worker.
  - **Parameters**:
    - `single_group_config` (Dict): The complete configuration dictionary needed to run `SingleGroupParserAgent`.
    - `prompts_json_path` (str): Path to the prompts JSON file.
  - **Returns**: Tuple `(group_name, final_state_object)`.
  - **Usage**: Submitted to the executor by `AllGroupsParserAgent.run`.

---

## File: `src/logllm/utils/chunk_manager.py`

*Note: This is a utility class but heavily used by agents.*

### Class: `ESTextChunkManager`
- **Purpose**: Manages the retrieval and chunking of text data (typically log lines) from Elasticsearch for analysis by agents, ensuring chunks adhere to specified token limits. It fetches all relevant documents upfront and provides chunks sequentially.
- **Key Methods**:
  - **`__init__(self, id: Any, field: str, index: str, db: ElasticsearchDatabase)`**
    - **Description**: Initializes by fetching all matching documents (hits) from ES based on ID, field, and index. Stores hits internally.
    - **Parameters**:
      - `id` (Any): Identifier (e.g., file ID, event ID) to match documents.
      - `field` (str): The field containing the text content (e.g., "content").
      - `index` (str): The Elasticsearch index to query.
      - `db` (ElasticsearchDatabase): Database instance.
    - **Usage**: `chunk_mgr = ESTextChunkManager(id=event_id, field="content", index=pre_process_index, db=db)`
  - **`get_next_chunk(self, max_len: int, len_fn: Callable[[str], int]) -> str`**
    - **Description**: Retrieves the next chunk of text based on the internal `start` pointer. Uses `_build_chunk` to dynamically assemble text from multiple hits while respecting the `max_len` token limit (using `len_fn`). Updates the internal `start` index for the subsequent call.
    - **Parameters**:
      - `max_len` (int): Maximum token length allowed for the chunk.
      - `len_fn` (Callable): Function to calculate token count (e.g., `model.token_count`).
    - **Returns**: A string containing the next chunk of text, or an empty string if all hits have been processed.
    - **Usage**: `chunk = chunk_mgr.get_next_chunk(max_tokens=10000, len_fn=model.token_count)`
  - **`is_end(self) -> bool`**: Checks if all fetched hits have been processed into chunks.
  - **`get_current_chunk(self) -> str | None`**: Returns the most recently generated chunk without advancing the pointer.
  - **`_build_chunk(...) -> str`**: Internal helper to construct a chunk by adding hits incrementally, halving the number of hits added if `max_len` is exceeded, until the limit is reached or no more hits are available.
  - **`_get_all_hits() -> list`**: Internal helper called by `__init__` to fetch all relevant documents using the ES scroll API.

